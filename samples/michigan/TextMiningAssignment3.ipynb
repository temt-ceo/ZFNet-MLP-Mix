{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "_You are currently looking at **version 1.1** of this notebook. To download notebooks and datafiles, as well as get help on Jupyter notebooks in the Coursera platform, visit the [Jupyter Notebook FAQ](https://www.coursera.org/learn/python-text-mining/resources/d9pwm) course resource._\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3\n",
    "\n",
    "In this assignment you will explore text message data and create models to predict if a message is spam or not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>FreeMsg Hey there darling it's been 3 week's n...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Even my brother is not like to speak with me. ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>As per your request 'Melle Melle (Oru Minnamin...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>WINNER!! As a valued network customer you have...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Had your mobile 11 months or more? U R entitle...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target\n",
       "0  Go until jurong point, crazy.. Available only ...       0\n",
       "1                      Ok lar... Joking wif u oni...       0\n",
       "2  Free entry in 2 a wkly comp to win FA Cup fina...       1\n",
       "3  U dun say so early hor... U c already then say...       0\n",
       "4  Nah I don't think he goes to usf, he lives aro...       0\n",
       "5  FreeMsg Hey there darling it's been 3 week's n...       1\n",
       "6  Even my brother is not like to speak with me. ...       0\n",
       "7  As per your request 'Melle Melle (Oru Minnamin...       0\n",
       "8  WINNER!! As a valued network customer you have...       1\n",
       "9  Had your mobile 11 months or more? U R entitle...       1"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "spam_data = pd.read_csv('spam.csv')\n",
    "\n",
    "spam_data['target'] = np.where(spam_data['target']=='spam',1,0) # np.where: 条件に応じて値をセット。\n",
    "spam_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(spam_data['text'], \n",
    "                                                    spam_data['target'], \n",
    "                                                    random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "What percentage of the documents in `spam_data` are spam?\n",
    "\n",
    "*This function should return a float, the percent value (i.e. $ratio * 100$).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " # Q1: balanced(データの配分が均等か)かどうかを%表示で取得します。\n",
    "def answer_one():\n",
    "    \n",
    "    return len(spam_data[spam_data['target'] == 1]) / len(spam_data) * 100#Your answer here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.406317300789663"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_one()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "Fit the training data `X_train` using a Count Vectorizer with default parameters.\n",
    "\n",
    "What is the longest token in the vocabulary?\n",
    "\n",
    "*This function should return a string.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " # Q2: 最も単語リストの中で文字数が多いものを取得します\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def answer_two():\n",
    "    \n",
    "    vect_model = CountVectorizer().fit(X_train)\n",
    "    \n",
    "    # np.argmax: 最大値のindexを返す。 np.amax: 最大値を返す。ここではmax関数のkey引数にlenを渡してlongest wrdを取得する\n",
    "    \n",
    "    maxlen_word = max(vect_model.get_feature_names(), key = len)\n",
    "    print(len(maxlen_word))\n",
    "    return maxlen_word #Your answer here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'com1win150ppmx3age16subscription'"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_two()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "Fit and transform the training data `X_train` using a Count Vectorizer with default parameters.\n",
    "\n",
    "Next, fit a fit a multinomial Naive Bayes classifier model with smoothing `alpha=0.1`. Find the area under the curve (AUC) score using the transformed test data.\n",
    "\n",
    "*This function should return the AUC score as a float.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Q3: AUCスコアを求める(CountVectorizer & Naive Bayes)\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def answer_three():\n",
    "    \n",
    "    # 3-1 Fit and transform the training data X_train using a Count Vectorizer with default parameters.\n",
    "    vect = CountVectorizer().fit(X_train)\n",
    "    X_train_vectorized = vect.transform(X_train)\n",
    "    \n",
    "    # 3-2  fit a multinomial Naive Bayes classifier model with smoothing alpha=0.1. \n",
    "    model = MultinomialNB(alpha = 0.1).fit(X_train_vectorized, y_train)\n",
    "    \n",
    "    # 3-3  Find the area under the curve (AUC) score using the transformed test data\n",
    "    X_test_vectorized = vect.transform(X_test) # ⇦ テスト時にはfitしない。transformのみ。\n",
    "    predictions = model.predict(X_test_vectorized)\n",
    "\n",
    "    \n",
    "    return roc_auc_score(y_test, predictions)#Your answer here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97208121827411165"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_three()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "\n",
    "Fit and transform the training data `X_train` using a Tfidf Vectorizer with default parameters.\n",
    "\n",
    "What 20 features have the smallest tf-idf and what 20 have the largest tf-idf?\n",
    "\n",
    "Put these features in a two series where each series is sorted by tf-idf value and then alphabetically by feature name. The index of the series should be the feature name, and the data should be the tf-idf.\n",
    "\n",
    "The series of 20 features with smallest tf-idfs should be sorted smallest tfidf first, the list of 20 features with largest tf-idfs should be sorted largest first. \n",
    "\n",
    "*This function should return a tuple of two series\n",
    "`(smallest tf-idfs series, largest tf-idfs series)`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Q4: TF−IDFを求める(大きいものから２０と小さいものから20)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def answer_four():\n",
    "    \n",
    "    # 4-1 Fit and transform the training data X_train using a Tfidf Vectorizer with default parameters.\n",
    "    vect = TfidfVectorizer().fit(X_train)\n",
    "    X_train_vectorized = vect.transform(X_train)\n",
    "    #print(X_train_vectorized)\n",
    "    \"\"\"\n",
    "     出力:\n",
    "      (0, 7305)\t0.198150664334\n",
    "      (0, 7085)\t0.352009658683\n",
    "      (0, 6421)\t0.390987576266\n",
    "      (0, 4624)\t0.471125306271\n",
    "                  :                   :\n",
    "      (4178, 3336)\t0.179845226367\n",
    "      (4178, 3199)\t0.275090466584\n",
    "      (4178, 2065)\t0.347443339363\n",
    "      (4178, 931)\t0.241310280193\n",
    "      (4178, 790)\t0.225106503401\n",
    "    \"\"\"    \n",
    "    \n",
    "    # 4-2 単語リスト取得\n",
    "    feature_names = np.array(vect.get_feature_names())\n",
    "#     print(len(feature_names)) ⇨ 7354\n",
    "    \n",
    "    # 4-3 What 20 features have the smallest tf-idf and what 20 have the largest tf-idf?\n",
    "    # maxを求めてsortすればminも求まる。(逆も然り)\n",
    "    \"\"\"\n",
    "    次のロジックの説明。\n",
    "    This max method is applied on a sparse matrix,\n",
    "    so you'd have to look up the SciPy documentation to understand exactly how it works.\n",
    "    \n",
    "    But essentially, that 0 is passed to the method as the axis parameter.\n",
    "    The axis 0 represent rows... so that would give you the max values in the sparse matrix for each row of said matrix.\n",
    "    \"\"\"\n",
    "#     print(X_train_vectorized.max(axis=0) .shape, X_train_vectorized.max(axis=1) .shape) # 確認用　⇨ (1, 7354) (4179, 1)\n",
    "    tfidf = X_train_vectorized.max(axis=0) # ⇦ <1x7354 sparse matrix of type '<class 'numpy.float64'>' with 0 stored elements in COOrdinate format>\n",
    "    print(tfidf.shape)\n",
    "    tfidf_a = tfidf.toarray().squeeze() # 階層が深いので[-1]以外使わない。(最後の並びのみ取る)\n",
    "    sorted_index = tfidf_a.argsort() # 並び替えをする。maxの場合: array([6305, 3196,  689, ..., 3293, 3180,  274]) (minの場合: array([ 0, 4908, 4907, ..., 2444, 2442, 7353]) )\n",
    "    print(sorted_index.shape)\n",
    "    \n",
    "#     print(len(sorted_index)) ⇨ 7354\n",
    "#     print(feature_names[sorted_index]) ⇨  ['sympathetic' 'healer' 'aaniye' ..., 'home' 'havent' '146tf150p']\n",
    "    \n",
    "    smallest_tfidf_values = feature_names[sorted_index][:20]      # tfidfの最も大きいword20ずつを取得する\n",
    "    largest_tfidf_values = feature_names[sorted_index][-20:]     # tfidfの最も小さいword20ずつを取得する\n",
    "    \n",
    "    # 低いtf-idfを示す用語はほぼどのような文書にも出てくる用語か、長い文章の中にしか登場しない用語(spamとの相関は関係無い)\n",
    "    ans1 = pd.Series(tfidf_a[sorted_index][:20], index = smallest_tfidf_values)\n",
    "    # 高いtf-idfを示す用語は特定の文書で頻出する用語(spamとの相関は関係無い)\n",
    "    ans2 = pd.Series(tfidf_a[sorted_index][-20:], index = largest_tfidf_values)\n",
    "    \n",
    "    return ans1, ans2#Your answer here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 7354)\n",
      "(7354,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(sympathetic     0.074475\n",
       " healer          0.074475\n",
       " aaniye          0.074475\n",
       " dependable      0.074475\n",
       " companion       0.074475\n",
       " listener        0.074475\n",
       " athletic        0.074475\n",
       " exterminator    0.074475\n",
       " psychiatrist    0.074475\n",
       " pest            0.074475\n",
       " determined      0.074475\n",
       " chef            0.074475\n",
       " courageous      0.074475\n",
       " stylist         0.074475\n",
       " psychologist    0.074475\n",
       " organizer       0.074475\n",
       " pudunga         0.074475\n",
       " venaam          0.074475\n",
       " diwali          0.091250\n",
       " mornings        0.091250\n",
       " dtype: float64, blank        0.932702\n",
       " tick         0.980166\n",
       " 645          1.000000\n",
       " done         1.000000\n",
       " too          1.000000\n",
       " anytime      1.000000\n",
       " beerage      1.000000\n",
       " where        1.000000\n",
       " ok           1.000000\n",
       " thank        1.000000\n",
       " yup          1.000000\n",
       " nite         1.000000\n",
       " lei          1.000000\n",
       " anything     1.000000\n",
       " er           1.000000\n",
       " thanx        1.000000\n",
       " okie         1.000000\n",
       " home         1.000000\n",
       " havent       1.000000\n",
       " 146tf150p    1.000000\n",
       " dtype: float64)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_four()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "\n",
    "Fit and transform the training data `X_train` using a Tfidf Vectorizer ignoring terms that have a document frequency strictly lower than **3**.\n",
    "\n",
    "Then fit a multinomial Naive Bayes classifier model with smoothing `alpha=0.1` and compute the area under the curve (AUC) score using the transformed test data.\n",
    "\n",
    "*This function should return the AUC score as a float.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Q5: AUCスコアを求める(TfidfVectorizer & Naive Bayes)(Q3とあまり変わらない)\n",
    "def answer_five():\n",
    "    # Fit and transform the training data X_train using a Tfidf Vectorizer ignoring terms that have a document frequency strictly lower than 3.\n",
    "    vect = TfidfVectorizer(min_df=3).fit(X_train)\n",
    "    X_train_vectorized = vect.transform(X_train)\n",
    "    model = MultinomialNB(alpha = 0.1).fit(X_train_vectorized, y_train)\n",
    "    X_test_vectorized = vect.transform(X_test)\n",
    "    predictions = model.predict(X_test_vectorized)\n",
    "    return roc_auc_score(y_test, predictions)#Your answer here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.94162436548223349"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_five()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "\n",
    "What is the average length of documents (number of characters) for not spam and spam documents?\n",
    "\n",
    "*This function should return a tuple (average length not spam, average length spam).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Q6: 文書の文字数の平均を求める\n",
    "def answer_six():\n",
    "#     spam_data['target'] = np.where(spam_data['target']=='spam',1,0) # np.where: 条件に応じて値をセット。\n",
    "    \n",
    "    spam_data['length'] = spam_data['text'].apply(len)\n",
    "\n",
    "    print(spam_data.head(10))\n",
    "    print(list(spam_data.iloc[1]['text'].split())) # word数は6つのみだが、文字数29は合ってる。\n",
    "    print(len(list(spam_data.iloc[1]['text'])))\n",
    "\n",
    "    df_spam_len = spam_data[spam_data['target'] == 1]['length'].mean()\n",
    "    df_no_spam_len = spam_data[spam_data['target'] == 0]['length'].mean()\n",
    "    \n",
    "    return df_no_spam_len, df_spam_len#Your answer here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  target  length\n",
      "0  Go until jurong point, crazy.. Available only ...       0     111\n",
      "1                      Ok lar... Joking wif u oni...       0      29\n",
      "2  Free entry in 2 a wkly comp to win FA Cup fina...       1     155\n",
      "3  U dun say so early hor... U c already then say...       0      49\n",
      "4  Nah I don't think he goes to usf, he lives aro...       0      61\n",
      "5  FreeMsg Hey there darling it's been 3 week's n...       1     148\n",
      "6  Even my brother is not like to speak with me. ...       0      77\n",
      "7  As per your request 'Melle Melle (Oru Minnamin...       0     160\n",
      "8  WINNER!! As a valued network customer you have...       1     158\n",
      "9  Had your mobile 11 months or more? U R entitle...       1     154\n",
      "['Ok', 'lar...', 'Joking', 'wif', 'u', 'oni...']\n",
      "29\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(71.023626943005183, 138.8661311914324)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_six()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "The following function has been provided to help you combine new features into the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_feature(X, feature_to_add):\n",
    "    \"\"\"\n",
    "    Returns sparse feature matrix with added feature.\n",
    "    feature_to_add can also be a list of features.\n",
    "    \"\"\"\n",
    "    from scipy.sparse import csr_matrix, hstack\n",
    "    return hstack([X, csr_matrix(feature_to_add).T], 'csr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7\n",
    "\n",
    "Fit and transform the training data X_train using a Tfidf Vectorizer ignoring terms that have a document frequency strictly lower than **5**.\n",
    "\n",
    "Using this document-term matrix and an additional feature, **the length of document (number of characters)**, fit a Support Vector Classification model with regularization `C=10000`. Then compute the area under the curve (AUC) score using the transformed test data.\n",
    "\n",
    "*This function should return the AUC score as a float.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Q7: AUCスコアを求める(TfidfVectorizer & SVM)\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "def answer_seven():\n",
    "    \n",
    "    # 7-1 Fit and transform the training data X_train using a Tfidf Vectorizer ignoring terms that have a document frequency strictly lower than 5.\n",
    "    vect = TfidfVectorizer(min_df=5).fit(X_train)\n",
    "    X_train_vectorized = vect.transform(X_train)\n",
    "    \n",
    "    # 7-2 Using this document-term matrix and an additional feature, the length of document (number of characters)\n",
    "    X_train_vectorized = add_feature(X_train_vectorized, X_train.apply(len))  # ⇦　 X_train.apply(len)は X_train.str.len()でもいいらしい\n",
    "    \n",
    "    # 7-3 fit a Support Vector Classification model with regularization C=10000\n",
    "    model = SVC(C = 10000).fit(X_train_vectorized, y_train) # C: regularization(小さいほどよりregularizeする)\n",
    "    \n",
    "    # 7-4 compute the area under the curve (AUC) score using the transformed test data\n",
    "    X_test_vectorized = vect.transform(X_test)\n",
    "    X_test_vectorized = add_feature(X_test_vectorized, X_test.apply(len))\n",
    "    predictions = model.predict(X_test_vectorized)\n",
    "    return roc_auc_score(y_test, predictions)#Your answer here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95813668234215565"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_seven()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8\n",
    "\n",
    "What is the average number of digits per document for not spam and spam documents?\n",
    "\n",
    "*This function should return a tuple (average # digits not spam, average # digits spam).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Q8: 文書中の数字の出現回数平均を求める\n",
    "def answer_eight():\n",
    "    \n",
    "    print(list(spam_data.iloc[2]['text'].split()))\n",
    "    print(spam_data[:3]['text'].str.findall('(\\d+)'))\n",
    "    print(spam_data[:3]['text'].str.findall('(\\d)').str.len())\n",
    "    \n",
    "    spam_data['digit_len'] = spam_data['text'].str.findall('(\\d)').str.len() # 数字の出現回数なので+が不要\n",
    "\n",
    "    df_spam_len = spam_data[spam_data['target'] == 1]['digit_len'].mean()\n",
    "    df_no_spam_len = spam_data[spam_data['target'] == 0]['digit_len'].mean()\n",
    "    \n",
    "    return df_no_spam_len, df_spam_len #Your answer here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Free', 'entry', 'in', '2', 'a', 'wkly', 'comp', 'to', 'win', 'FA', 'Cup', 'final', 'tkts', '21st', 'May', '2005.', 'Text', 'FA', 'to', '87121', 'to', 'receive', 'entry', 'question(std', 'txt', \"rate)T&C's\", 'apply', \"08452810075over18's\"]\n",
      "0                                       []\n",
      "1                                       []\n",
      "2    [2, 21, 2005, 87121, 08452810075, 18]\n",
      "Name: text, dtype: object\n",
      "0     0\n",
      "1     0\n",
      "2    25\n",
      "Name: text, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.29927461139896372, 15.759036144578314)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_eight()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 9\n",
    "\n",
    "Fit and transform the training data `X_train` using a Tfidf Vectorizer ignoring terms that have a document frequency strictly lower than **5** and using **word n-grams from n=1 to n=3** (unigrams, bigrams, and trigrams).\n",
    "\n",
    "Using this document-term matrix and the following additional features:\n",
    "* the length of document (number of characters)\n",
    "* **number of digits per document**\n",
    "\n",
    "fit a Logistic Regression model with regularization `C=100`. Then compute the area under the curve (AUC) score using the transformed test data.\n",
    "\n",
    "*This function should return the AUC score as a float.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Q9: AUCスコアを求める(TfidfVectorizer & LogisticRegression)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def answer_nine():\n",
    "    \n",
    "    # 9-1 Fit and transform the training data X_train using a Tfidf Vectorizer ignoring terms that have a document frequency strictly lower than 5\n",
    "    # and using word n-grams from n=1 to n=3 (unigrams, bigrams, and trigrams).\n",
    "    vect = TfidfVectorizer(min_df=5, ngram_range=(1,3)).fit(X_train)\n",
    "    X_train_vectorized = vect.transform(X_train)\n",
    "    \n",
    "    # 9-2 Using this document-term matrix and an additional feature, the length of document (number of characters)\n",
    "    # and number of digits per document\n",
    "    X_train_vectorized = add_feature(X_train_vectorized, X_train.apply(len))  # ⇦　 X_train.apply(len)は X_train.str.len()でもいいらしい\n",
    "    X_train_vectorized = add_feature(X_train_vectorized, X_train.str.findall('(\\d)').str.len())\n",
    "    \n",
    "    # 9-3 fit a Logistic Regression model with regularization C=100\n",
    "    model = LogisticRegression(C = 100).fit(X_train_vectorized, y_train) # C: regularization(小さいほどよりregularizeする)\n",
    "    \n",
    "    # 9-4 compute the area under the curve (AUC) score using the transformed test data\n",
    "    X_test_vectorized = vect.transform(X_test)\n",
    "    X_test_vectorized = add_feature(X_test_vectorized, X_test.apply(len))\n",
    "    X_test_vectorized = add_feature(X_test_vectorized, X_test.str.findall('(\\d)').str.len())\n",
    "\n",
    "    predictions = model.predict(X_test_vectorized)\n",
    "    return roc_auc_score(y_test, predictions) #Your answer here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96533283533945646"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_nine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 10\n",
    "\n",
    "What is the average number of non-word characters (anything other than a letter, digit or underscore) per document for not spam and spam documents?\n",
    "\n",
    "*Hint: Use `\\w` and `\\W` character classes*\n",
    "\n",
    "*This function should return a tuple (average # non-word characters not spam, average # non-word characters spam).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Q10: 文書中の文字以外の出現回数平均を求める\n",
    "def answer_ten():\n",
    "    \n",
    "    # 10-1 What is the average number of non-word characters per document for not spam and spam documents?\n",
    "    print(list(spam_data[:3]['text'].str.split()))\n",
    "    print(spam_data[:3]['text'].str.findall('(\\W+)'))\n",
    "    print(spam_data[:3]['text'].str.findall('(\\W)').str.len())\n",
    "    \n",
    "    spam_data['nonword_len'] = spam_data['text'].str.findall('(\\W)').str.len() # 数字の出現回数なので+が不要\n",
    "\n",
    "    df_spam_len = spam_data[spam_data['target'] == 1]['nonword_len'].mean()\n",
    "    df_no_spam_len = spam_data[spam_data['target'] == 0]['nonword_len'].mean()\n",
    "    \n",
    "    return df_no_spam_len, df_spam_len#Your answer here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Go', 'until', 'jurong', 'point,', 'crazy..', 'Available', 'only', 'in', 'bugis', 'n', 'great', 'world', 'la', 'e', 'buffet...', 'Cine', 'there', 'got', 'amore', 'wat...'], ['Ok', 'lar...', 'Joking', 'wif', 'u', 'oni...'], ['Free', 'entry', 'in', '2', 'a', 'wkly', 'comp', 'to', 'win', 'FA', 'Cup', 'final', 'tkts', '21st', 'May', '2005.', 'Text', 'FA', 'to', '87121', 'to', 'receive', 'entry', 'question(std', 'txt', \"rate)T&C's\", 'apply', \"08452810075over18's\"]]\n",
      "0    [ ,  ,  , , , .. ,  ,  ,  ,  ,  ,  ,  ,  ,  , ...\n",
      "1                              [ , ... ,  ,  ,  , ...]\n",
      "2    [ ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  , ...\n",
      "Name: text, dtype: object\n",
      "0    28\n",
      "1    11\n",
      "2    33\n",
      "Name: text, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(17.291813471502589, 29.041499330655956)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_ten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 11\n",
    "\n",
    "Fit and transform the training data X_train using a Count Vectorizer ignoring terms that have a document frequency strictly lower than **5** and using **character n-grams from n=2 to n=5.**\n",
    "\n",
    "To tell Count Vectorizer to use character n-grams pass in `analyzer='char_wb'` which creates character n-grams only from text inside word boundaries. This should make the model more robust to spelling mistakes.\n",
    "\n",
    "Using this document-term matrix and the following additional features:\n",
    "* the length of document (number of characters)\n",
    "* number of digits per document\n",
    "* **number of non-word characters (anything other than a letter, digit or underscore.)**\n",
    "\n",
    "fit a Logistic Regression model with regularization C=100. Then compute the area under the curve (AUC) score using the transformed test data.\n",
    "\n",
    "Also **find the 10 smallest and 10 largest coefficients from the model** and return them along with the AUC score in a tuple.\n",
    "\n",
    "The list of 10 smallest coefficients should be sorted smallest first, the list of 10 largest coefficients should be sorted largest first.\n",
    "\n",
    "The three features that were added to the document term matrix should have the following names should they appear in the list of coefficients:\n",
    "['length_of_doc', 'digit_count', 'non_word_char_count']\n",
    "\n",
    "*This function should return a tuple `(AUC score as a float, smallest coefs list, largest coefs list)`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def answer_eleven():\n",
    "    \n",
    "    # 11-1 Fit and transform the training data X_train using a Count Vectorizer ignoring terms that have a document frequency strictly lower than 5\n",
    "    # and using word n-grams from n=2 to n=5 (unigrams, bigrams, and trigrams). To tell Count Vectorizer to use character n-grams\n",
    "    # pass in analyzer='char_wb' which creates character n-grams only from text inside word boundaries.\n",
    "    vect = CountVectorizer(min_df=5, ngram_range=(2, 5), analyzer='char_wb').fit(X_train)\n",
    "    X_train_vectorized = vect.transform(X_train)\n",
    "    \n",
    "    # 11-2 Using this document-term matrix and an additional feature, the length of document (number of characters)\n",
    "    # and number of digits per document and number of non-word characters\n",
    "    X_train_vectorized = add_feature(X_train_vectorized, X_train.apply(len))  # ⇦　 X_train.apply(len)は X_train.str.len()でもいいらしい\n",
    "    X_train_vectorized = add_feature(X_train_vectorized, X_train.str.findall('(\\d)').str.len())\n",
    "    X_train_vectorized = add_feature(X_train_vectorized, X_train.str.findall('(\\W)').str.len())\n",
    "    \n",
    "    # 11-3 fit a Logistic Regression model with regularization C=100\n",
    "    model = LogisticRegression(C = 100).fit(X_train_vectorized, y_train) # C: regularization(小さいほどよりregularizeする)\n",
    "    \n",
    "    # 11-4 compute the area under the curve (AUC) score using the transformed test data\n",
    "    X_test_vectorized = vect.transform(X_test)\n",
    "    X_test_vectorized = add_feature(X_test_vectorized, X_test.apply(len))\n",
    "    X_test_vectorized = add_feature(X_test_vectorized, X_test.str.findall('(\\d)').str.len())\n",
    "    X_test_vectorized = add_feature(X_test_vectorized, X_test.str.findall('(\\W)').str.len())\n",
    "\n",
    "    predictions = model.predict(X_test_vectorized)\n",
    "    auc = roc_auc_score(y_test, predictions)\n",
    "    \n",
    "    # 11-5 スパムと高い相関値を示す用語、低い相関値を示す用語を抽出する。\n",
    "    sorted_coef_index = model.coef_[0].argsort() # 並び替えをする\n",
    "    feature_names = np.array(vect.get_feature_names())\n",
    "    \n",
    "    # 11-6 11-4にてfeatureを３つ追加しているので対応する ['length_of_doc', 'digit_count', 'non_word_char_count']\n",
    "    feature_names = np.append(feature_names, ['length_of_doc', 'digit_count', 'non_word_char_count'])\n",
    "    print(len(feature_names), len(sorted_coef_index))\n",
    "    print('Smallest Coefs:\\n{}\\n'.format(feature_names[sorted_coef_index][:10]))\n",
    "    print('Largest Coefs:\\n{}\\n'.format(feature_names[sorted_coef_index][-10:]))\n",
    "    \n",
    "    # 11-7 スパムと低い相関値を示す用語、高い相関値を示す用語をそれぞれ低い順と高い順に10個並べて返す\n",
    "    \"\"\"\n",
    "    [::n] creates a list containing every n-th element of the original list, starting with the zeroth element.\n",
    "    [::-n] does the same backwards, starting with the last element.\n",
    "    tolist()を使うことで純粋なlistに変換する。\n",
    "    \"\"\"\n",
    "    return auc, feature_names[sorted_coef_index][:10].tolist(), feature_names[sorted_coef_index][:-11:-1].tolist()#Your answer here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16317 16317\n",
      "Smallest Coefs:\n",
      "['. ' '..' '? ' ' i' ' y' ' go' ':)' ' h' 'go' ' m']\n",
      "\n",
      "Largest Coefs:\n",
      "['ar' 'ww' ' x' 'mob' ' ch' 'xt' 'co' 'ia' 'ne' 'digit_count']\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.97885931107074342,\n",
       " ['. ', '..', '? ', ' i', ' y', ' go', ':)', ' h', 'go', ' m'],\n",
       " ['digit_count', 'ne', 'ia', 'co', 'xt', ' ch', 'mob', ' x', 'ww', 'ar'])"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_eleven()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "python-text-mining",
   "graded_item_id": "Pn19K",
   "launcher_item_id": "y1juS",
   "part_id": "ctlgo"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
