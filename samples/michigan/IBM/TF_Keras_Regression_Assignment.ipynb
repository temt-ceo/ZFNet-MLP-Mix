{"cells":[{"metadata":{},"cell_type":"markdown","source":"### ◉research question\n**Improving Newral Network by modifying hyper parameter and layers**\n* I will test how the results are changing by changing a series of model patterns using keras.\n* The dataset is the volumes of construction materials which is used to compose concrete, such as cement and water.\n* The strength of concrete is the target value, and I will predict and research the y-hat value by regression model and mean_squared_error value.\n* This dataset also includes age of concrete mix even though age is important value for strength.\n\n### ◉the region and the domain category that this data sets are about\n**USA, construction materials**\n"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#### Download the concrete data\nconcrete_data = pd.read_csv('../input/us-concrete-data/concrete_data.csv')\nprint(concrete_data.shape)\nprint(concrete_data.head()) # By the way, unit is 'cubic meter' and days old of concrete mix, and unit of strength is MPa.\nconcrete_data.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"concrete_data.isnull().sum() # Looks very clean data.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = concrete_data\ncols = df.columns\nX = df[cols[cols != 'Strength']]\ny = df['Strength']","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"# To get reproducible results I'm setting random seed\nnp.random.seed(1)\ntf.random.set_seed(1)\nimport keras\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom keras.models import Sequential\nfrom keras.layers import Dense\ntest_size = 0.3\ndef random_data_split(X, y, seed):\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=seed)\n    return X_train, X_test, y_train, y_test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Step 1. Without Normalization"},{"metadata":{"trusted":true},"cell_type":"code","source":"# baseline model (One hidden layer 10 nodes, 50 epochs)\nmse_list = []\npredicted_list = {}\ndef create_baseline_model():\n    baseline_model = Sequential()\n    baseline_model.add(Dense(10, activation='relu', input_shape=(X.shape[1],)))\n    baseline_model.add(Dense(1))\n    baseline_model.compile(optimizer='adam', loss='mean_squared_error')\n    return baseline_model\n\n# collect 50 mse values.\nfor i in range(50):\n    if (i + 1) % 10 == 0:\n        print('Now {} times calculating.'.format(i + 1))\n    model = create_baseline_model()\n    X_train, X_test, y_train, y_test = random_data_split(X, y, i)\n    model.fit(X_train, y_train, epochs=50, verbose=0)\n    y_hats = model.predict(X_test)\n    mse = mean_squared_error(y_test, y_hats)\n    mse_list.append(mse)\n    predicted_list[i] = {'y_test': y_test, 'y_hats': y_hats}\n\n# Calculate mean and standard deviation of mse values\nmse_mean = np.mean(mse_list)\nmse_std = np.std(mse_list)\nprint('Mean value of MSE:{:.2f}, Standard Deviation value of MSE:{:.2f}.'.format(mse_mean, mse_std), 'First three MSE values:', mse_list[0:3])\nprint('Acctual Value samples', predicted_list[0]['y_test'].values[0:3], 'Predicted Value samples', np.around(predicted_list[0]['y_hats'].flatten()[0:3], decimals=2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Step 2. With Normalization"},{"metadata":{"trusted":true},"cell_type":"code","source":"# In this time, I'm not doing any Normalization. So I'll Normalize continuous values (by subtracting the mean from the individual predictors and dividing by the standard deviation).\nX_norm = (X - X.mean()) / X.std()\nX_norm.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now I'm getting ready to examin how normalization can improve the baseline model (One hidden layer 10 nodes, 50 epochs)\nmse_list_norm = []\npredicted_list_norm = {}\n# collect 50 mse values.\nfor i in range(50):\n    if (i + 1) % 10 == 0:\n        print('Now {} times calculating.'.format(i + 1))\n    model = create_baseline_model()\n    X_train, X_test, y_train, y_test = random_data_split(X_norm, y, i)\n    model.fit(X_train, y_train, epochs=50, verbose=0)\n    y_hats = model.predict(X_test)\n    mse = mean_squared_error(y_test, y_hats)\n    mse_list_norm.append(mse)\n    predicted_list_norm[i] = {'y_test': y_test, 'y_hats': y_hats}\n\n# Calculate mean and standard deviation of mse values\nmse_mean_norm = np.mean(mse_list_norm)\nmse_std_norm = np.std(mse_list_norm)\nprint('Mean value of MSE:{:.2f} and Standard Deviation value of MSE:{:.2f}.'.format(mse_mean_norm, mse_std_norm), 'First three MSE values:', mse_list_norm[0:3])\nprint('Acctual Value samples', predicted_list_norm[0]['y_test'].values[0:3], 'Predicted Value samples', np.around(predicted_list_norm[0]['y_hats'].flatten()[0:3], decimals=2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### ■In my case, after I applied Normalization to X, I can get MSE value of 368 over the same baseline model.<br>So mean of 50 MSE values are only slightly getting better from 369 to 368.<br>But Standard Deviation is reducing dynamically from 404 to 109.<br>So I checked histgram of 50 MSE values. Look at following charts."},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.figure()\nfig, ((ax1), (ax2)) = plt.subplots(1, 2, sharex=True, sharey=True)\nax1.hist(mse_list, alpha=0.5, bins=20, color='r', label='baseline model')\nax2.hist(mse_list_norm, alpha=0.5, bins=5, color='b', label='After applied Normalization')\nax1.legend()\nax2.legend()\nax1.set_xlabel('Mean Squared Values of baseline model')\nax2.set_xlabel('Mean Squared Values after the Normalization')\nax1.set_ylabel('Standard Deviation value of MSE')\nfig= plt.gcf()\nfig.set_size_inches(10, 5.5)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### In this charts, I created histgram of each mean squared values. By looking at above charts, we can recognize MSE values are no longer over the 1000 after being applied Normalization.<br>By using Normalization it seems we can get narrow range of Mean Squared Error values.<br>In other words, doing Normlization to continuous values are stabilizing Mean Squared Values.<br>In this time, I'm calculating MSE value 50 times, so applying Normalization is very important if I calculate only 1 MSE value. And usually we calculate MSE value only one time.\n**To recap, Normalization is essential to get correct Mean Square Value.**"},{"metadata":{},"cell_type":"markdown","source":"## Step 3. Increase Epochs"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Then I will increase epoch values to 100 and look at how models are improved by increasing epoch.\n# Now I'm getting ready to examin how increasing epochs can improve the normalized model (One hidden layer 10 nodes, 100 epochs)\nepochs = 100\nmse_list_double_epoch = []\npredicted_list_double_epoch = {}\n# collect 50 mse values.\nfor i in range(50):\n    if (i + 1) % 10 == 0:\n        print('Now {} times calculating.'.format(i + 1))\n    model = create_baseline_model()\n    X_train, X_test, y_train, y_test = random_data_split(X_norm, y, i)\n    model.fit(X_train, y_train, epochs=epochs, verbose=0)\n    y_hats = model.predict(X_test)\n    mse = mean_squared_error(y_test, y_hats)\n    mse_list_double_epoch.append(mse)\n    predicted_list_double_epoch[i] = {'y_test': y_test, 'y_hats': y_hats}\n\n# Calculate mean and standard deviation of mse values\nmse_mean_double_epoch = np.mean(mse_list_double_epoch)\nmse_std_double_epoch = np.std(mse_list_double_epoch)\nprint('Mean value of MSE:{:.2f} and Standard Deviation value of MSE:{:.2f}.'.format(mse_mean_double_epoch, mse_std_double_epoch), 'First three MSE values:', mse_list_double_epoch[0:3])\nprint('Acctual Value samples', predicted_list_double_epoch[0]['y_test'].values[0:3], 'Predicted Value samples', np.around(predicted_list_double_epoch[0]['y_hats'].flatten()[0:3], decimals=2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### ■By increasing epochs to 100, mean of 50 Mean Squared value is improving to 168 from 368. There is no wonder though."},{"metadata":{},"cell_type":"markdown","source":"## Step 4. Increase NN Layers and Set Epochs Back to Step 2."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Then I will increase hidden layers to three but set epochs back to 50 same as B.\n# Now I'm getting ready to examin how increasing hidden layers can improve the normalized model (Three hidden layer 10 nodes, 50 epochs)\nepochs = 50\nmse_list_three_layers = []\npredicted_list_three_layers = {}\n\ndef create_three_layer_model():\n    baseline_model = Sequential()\n    baseline_model.add(Dense(10, activation='relu', input_shape=(X.shape[1],)))\n    baseline_model.add(Dense(10, activation='relu'))\n    baseline_model.add(Dense(10, activation='relu'))\n    baseline_model.add(Dense(1))\n    baseline_model.compile(optimizer='adam', loss='mean_squared_error')\n    return baseline_model\n\n# collect 50 mse values.\nfor i in range(50):\n    if (i + 1) % 10 == 0:\n        print('Now {} times calculating.'.format(i + 1))\n    model = create_three_layer_model()\n    X_train, X_test, y_train, y_test = random_data_split(X_norm, y, i)\n    model.fit(X_train, y_train, epochs=epochs, verbose=0)\n    y_hats = model.predict(X_test)\n    mse = mean_squared_error(y_test, y_hats)\n    mse_list_three_layers.append(mse)\n    predicted_list_three_layers[i] = {'y_test': y_test, 'y_hats': y_hats}\n\n# Calculate mean and standard deviation of mse values\nmse_mean_three_layers = np.mean(mse_list_three_layers)\nmse_std_three_layers = np.std(mse_list_three_layers)\nprint('Mean value of MSE:{:.2f} and Standard Deviation value of MSE:{:.2f}.'.format(mse_mean_three_layers, mse_std_three_layers), 'First three MSE values:', mse_list_three_layers[0:3])\nprint('Acctual Value samples', predicted_list_three_layers[0]['y_test'].values[0:3], 'Predicted Value samples', np.around(predicted_list_three_layers[0]['y_hats'].flatten()[0:3], decimals=2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### ■By increasing hidden layer to three from one, mean of 50 Mean Squared value is improving to 124 from 368. Even though epochs are same as step 2, there is a big change to MSE value to be improved."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}